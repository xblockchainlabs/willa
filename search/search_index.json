{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A streaming microservice framework for Node.js Overview \uf0c1 A streaming microservice framework for Node.js with which you can perform vairous operations on real-time data like transformation,aggregation,validation etc. Getting Started \uf0c1 A typical wi|las pipeline must consist of a source and a sink. A typical source & sink represents the input stream and output sink repectively. Currently Kafka and node streams are supported. Data transformation takes place in pipeline flows , their is no restriction in number of flows which can be added to a pipeline however their can be only on Source and one sink. Pipeline \uf0c1 A wi|las pipeline can be crated by following code snipet. const { Pipeline } = require('@xblockchainlabs/willa'); const pipeline = new Pipeline(); Source \uf0c1 As disccussed above source represents the input to the pipeline i.e the source is the starting point of a pipeline , currently wi|la support 2 types of source source and sourceCommitable . source can be either be a Node Stream or a Kafka Stream whereas sourceCommitable currently supports only kafka streams. Inorder to attach a source/sourceCommitable to the pipeline use follow code snipet. Node Stream \uf0c1 pipeline.source(Stream.consumer(options)); Kafka Stream \uf0c1 pipeline.source(Kafka.consumer(options)); Kafka Stream (commitable) \uf0c1 pipeline.sourceCommitable(Kafka.consumer(options)); options \uf0c1 { name: String, errorStrategy: String ['reset' , 'uncommit' , 'report' ] } Note name is mandatory to be passed , whereas default errorStrategy is set to `reset` Flow \uf0c1 Flow represents or perform logical opreation for the transformation of the incomming data. Thier is no limitation on the number of flows which can be chainned in a pipeline however flow cannot be either first of last element of a pipeline. A typical wi|la flow accpect 2 types of operation i.e lamda function and the Batch processes. Lamda functions \uf0c1 .flow((data, err, next) => { // your custom logic next(data, err); }) Batch \uf0c1 Batch is used in the cases where you want pipeline to wait for certain time / number of data inputs and then perform your logical operation. wi|la provides two types of batch . Reduce .flow(Batch.reduce(options,reducerfunc,initailvalue) Map .flow(Batch.reduce(options,mapfunc) options { number: Number, timeout: Number (time in ms), groupBy: String || [String], attributes: String || [String] } Sink \uf0c1 As disccussed above sink represents the output/result to the pipeline i.e the sink is the end point of a pipeline. source can be either be a Node Stream or a Kafka Stream. Inorder to attach a sink to the pipeline use follow code snipet. .sink((data, err, next) => { next(data, err); }); Kafka .sink(Kafka.producer(options)); Starting the App \uf0c1 once we have created your pipeline you need to add it to the app . to do that use following code snipet. const { App , Pipeline } = require('@xblockchainlabs/willa'); const app = App(name ,options); app.add(pipeline); options { kafka: kafkaconfig }","title":"v1"},{"location":"#overview","text":"A streaming microservice framework for Node.js with which you can perform vairous operations on real-time data like transformation,aggregation,validation etc.","title":"Overview"},{"location":"#getting-started","text":"A typical wi|las pipeline must consist of a source and a sink. A typical source & sink represents the input stream and output sink repectively. Currently Kafka and node streams are supported. Data transformation takes place in pipeline flows , their is no restriction in number of flows which can be added to a pipeline however their can be only on Source and one sink.","title":"Getting Started"},{"location":"#pipeline","text":"A wi|las pipeline can be crated by following code snipet. const { Pipeline } = require('@xblockchainlabs/willa'); const pipeline = new Pipeline();","title":"Pipeline"},{"location":"#source","text":"As disccussed above source represents the input to the pipeline i.e the source is the starting point of a pipeline , currently wi|la support 2 types of source source and sourceCommitable . source can be either be a Node Stream or a Kafka Stream whereas sourceCommitable currently supports only kafka streams. Inorder to attach a source/sourceCommitable to the pipeline use follow code snipet.","title":"Source"},{"location":"#node-stream","text":"pipeline.source(Stream.consumer(options));","title":"Node Stream"},{"location":"#kafka-stream","text":"pipeline.source(Kafka.consumer(options));","title":"Kafka Stream"},{"location":"#kafka-stream-commitable","text":"pipeline.sourceCommitable(Kafka.consumer(options));","title":"Kafka Stream (commitable)"},{"location":"#options","text":"{ name: String, errorStrategy: String ['reset' , 'uncommit' , 'report' ] } Note name is mandatory to be passed , whereas default errorStrategy is set to `reset`","title":"options"},{"location":"#flow","text":"Flow represents or perform logical opreation for the transformation of the incomming data. Thier is no limitation on the number of flows which can be chainned in a pipeline however flow cannot be either first of last element of a pipeline. A typical wi|la flow accpect 2 types of operation i.e lamda function and the Batch processes.","title":"Flow"},{"location":"#lamda-functions","text":".flow((data, err, next) => { // your custom logic next(data, err); })","title":"Lamda functions"},{"location":"#batch","text":"Batch is used in the cases where you want pipeline to wait for certain time / number of data inputs and then perform your logical operation. wi|la provides two types of batch . Reduce .flow(Batch.reduce(options,reducerfunc,initailvalue) Map .flow(Batch.reduce(options,mapfunc) options { number: Number, timeout: Number (time in ms), groupBy: String || [String], attributes: String || [String] }","title":"Batch"},{"location":"#sink","text":"As disccussed above sink represents the output/result to the pipeline i.e the sink is the end point of a pipeline. source can be either be a Node Stream or a Kafka Stream. Inorder to attach a sink to the pipeline use follow code snipet. .sink((data, err, next) => { next(data, err); }); Kafka .sink(Kafka.producer(options));","title":"Sink"},{"location":"#starting-the-app","text":"once we have created your pipeline you need to add it to the app . to do that use following code snipet. const { App , Pipeline } = require('@xblockchainlabs/willa'); const app = App(name ,options); app.add(pipeline); options { kafka: kafkaconfig }","title":"Starting the App"},{"location":"about/contributing/","text":"","title":"Contributing"},{"location":"about/license/","text":"","title":"License"},{"location":"about/release-notes/","text":"","title":"Release notes"},{"location":"examples/stream/","text":"Node Stream \uf0c1 const { App, Pipelines, Stream, Batch } = require('../'); const pipeline = Pipelines(); pipeline.source(Stream.consumer({ name: 'process' })) .flow((data, err, next) => { let num = parseInt(data.num); Object.assign(data, { num: num + 1 , from : country[Math.floor(Math.random() * country.length)]}); Object.assign(data, { to : country[Math.floor(Math.random() * country.length)]}); next(data, err); }) .sink((data, err, next) => { console.log(JSON.stringify(data, null, 3)); next(data, err); }); const app = App('test'); app.add(pipeline); for (let i = 0; i < 7; i++) { app.writeStream('process', { num: i }); } Node Stream + Batch - Reduce \uf0c1 const { App, Pipelines, Stream, Batch } = require('../'); const pipeline = Pipelines(); pipeline.source(Stream.consumer({ name: 'process' })) .flow((data, err, next) => { let num = parseInt(data.num); Object.assign(data, { num: num + 1 , from : countries[Math.floor(Math.random() * country.length)]}); Object.assign(data, { to : countries[Math.floor(Math.random() * country.length)]}); // throw new Error('Kaka punjabi'); //err = new Error('Kaka punjabi'); next(data, err); }) .flow(Batch.reduce({ number: 5, timeout: 30000, groupBy: \"from\", attributes: [\"num\", \"to\"]}, (aggtr ,data) => { let num = parseInt(data.num); aggtr.number += num; return aggtr; }, { number:0})) .sink((data, err, next) => { console.log(JSON.stringify(data, null, 3)); next(data, err); }); const app = App('test'); app.add(pipeline); for (let i = 0; i < 7; i++) { app.writeStream('process', { num: i }); } Node Stream + Batch - Map \uf0c1 const { App, Pipelines, Stream, Batch } = require('../'); const pipeline = Pipelines(); pipeline.source(Stream.consumer({ name: 'process' })) .flow((data, err, next) => { let num = parseInt(data.num); Object.assign(data, { num: num + 1 , from : countries[Math.floor(Math.random() * country.length)]}); Object.assign(data, { to : countries[Math.floor(Math.random() * country.length)]}); next(data, err); }) .flow(Flowfunc.batch({ number: 5, timeout: 30000 }, (data, err, next) => { console.log(\"...\",data); let num = parseInt(data.num); Object.assign(data, { num: num - 1 }); return data; })) .sink((data, err, next) => { // console.log(JSON.stringify(data, null, 3)); next(data, err); }); const app = App('test'); app.add(pipeline); for (let i = 0; i < 7; i++) { app.writeStream('process', { num: i }); }","title":"v1"},{"location":"examples/stream/#node-stream","text":"const { App, Pipelines, Stream, Batch } = require('../'); const pipeline = Pipelines(); pipeline.source(Stream.consumer({ name: 'process' })) .flow((data, err, next) => { let num = parseInt(data.num); Object.assign(data, { num: num + 1 , from : country[Math.floor(Math.random() * country.length)]}); Object.assign(data, { to : country[Math.floor(Math.random() * country.length)]}); next(data, err); }) .sink((data, err, next) => { console.log(JSON.stringify(data, null, 3)); next(data, err); }); const app = App('test'); app.add(pipeline); for (let i = 0; i < 7; i++) { app.writeStream('process', { num: i }); }","title":"Node Stream"},{"location":"examples/stream/#node-stream-batch-reduce","text":"const { App, Pipelines, Stream, Batch } = require('../'); const pipeline = Pipelines(); pipeline.source(Stream.consumer({ name: 'process' })) .flow((data, err, next) => { let num = parseInt(data.num); Object.assign(data, { num: num + 1 , from : countries[Math.floor(Math.random() * country.length)]}); Object.assign(data, { to : countries[Math.floor(Math.random() * country.length)]}); // throw new Error('Kaka punjabi'); //err = new Error('Kaka punjabi'); next(data, err); }) .flow(Batch.reduce({ number: 5, timeout: 30000, groupBy: \"from\", attributes: [\"num\", \"to\"]}, (aggtr ,data) => { let num = parseInt(data.num); aggtr.number += num; return aggtr; }, { number:0})) .sink((data, err, next) => { console.log(JSON.stringify(data, null, 3)); next(data, err); }); const app = App('test'); app.add(pipeline); for (let i = 0; i < 7; i++) { app.writeStream('process', { num: i }); }","title":"Node Stream + Batch - Reduce"},{"location":"examples/stream/#node-stream-batch-map","text":"const { App, Pipelines, Stream, Batch } = require('../'); const pipeline = Pipelines(); pipeline.source(Stream.consumer({ name: 'process' })) .flow((data, err, next) => { let num = parseInt(data.num); Object.assign(data, { num: num + 1 , from : countries[Math.floor(Math.random() * country.length)]}); Object.assign(data, { to : countries[Math.floor(Math.random() * country.length)]}); next(data, err); }) .flow(Flowfunc.batch({ number: 5, timeout: 30000 }, (data, err, next) => { console.log(\"...\",data); let num = parseInt(data.num); Object.assign(data, { num: num - 1 }); return data; })) .sink((data, err, next) => { // console.log(JSON.stringify(data, null, 3)); next(data, err); }); const app = App('test'); app.add(pipeline); for (let i = 0; i < 7; i++) { app.writeStream('process', { num: i }); }","title":"Node Stream + Batch - Map"}]}